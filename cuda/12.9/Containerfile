# ODH CUDA Base Image (CentOS Stream 9)
# CUDA Version: 12.9
# Generated from Containerfile.cuda.template - edit template, not this file
#
# CentOS Stream 9 required: CUDA packages fail on UBI 9 (missing OpenGL/mesa deps)
# Usage: ./scripts/build.sh cuda-12.9
# Build args: cuda/12.9/app.conf

ARG BASE_IMAGE
ARG TARGETARCH
ARG CUDA_VERSION
ARG NV_CUDA_CUDART_VERSION
ARG NV_CUDA_LIB_VERSION
ARG NV_NVTX_VERSION
ARG NV_LIBNPP_VERSION
ARG NV_LIBCUBLAS_VERSION
ARG NV_LIBNCCL_VERSION
ARG NV_LIBNCCL_PACKAGE_VERSION
ARG NV_CUDNN_VERSION
ARG NVIDIA_REQUIRE_CUDA

# Build metadata for OCI labels
ARG BUILD_DATE
ARG VCS_REF=unknown
ARG VERSION=0.0.1

# -----------------------------------------------------------------------------
# Multi-arch base stages
# -----------------------------------------------------------------------------
FROM ${BASE_IMAGE} AS base
WORKDIR /opt/app-root/bin

FROM base AS cuda-base-amd64
ENV NVARCH=x86_64

FROM base AS cuda-base-arm64
ENV NVARCH=sbsa

FROM cuda-base-${TARGETARCH} AS cuda-base

ARG TARGETARCH
ARG NVIDIA_REQUIRE_CUDA
ARG CUDA_VERSION
ARG NV_CUDA_CUDART_VERSION
ARG NV_CUDA_LIB_VERSION
ARG NV_NVTX_VERSION
ARG NV_LIBNPP_VERSION
ARG NV_LIBCUBLAS_VERSION
ARG NV_LIBNCCL_VERSION
ARG NV_LIBNCCL_PACKAGE_VERSION
ARG NV_CUDNN_VERSION

USER 0
WORKDIR /opt/app-root/bin

# -----------------------------------------------------------------------------
# CUDA Version Environment
# Source: https://gitlab.com/nvidia/container-images/cuda/-/tree/master/dist
# -----------------------------------------------------------------------------
ENV NVIDIA_REQUIRE_CUDA=${NVIDIA_REQUIRE_CUDA}
ENV CUDA_VERSION=${CUDA_VERSION}
ENV NV_CUDA_CUDART_VERSION=${NV_CUDA_CUDART_VERSION}
ENV NV_CUDA_LIB_VERSION=${NV_CUDA_LIB_VERSION}
ENV NV_NVTX_VERSION=${NV_NVTX_VERSION}
ENV NV_LIBNPP_VERSION=${NV_LIBNPP_VERSION}
ENV NV_LIBNPP_PACKAGE=libnpp-12-9-${NV_LIBNPP_VERSION}
ENV NV_LIBCUBLAS_VERSION=${NV_LIBCUBLAS_VERSION}
ENV NV_LIBNCCL_PACKAGE_NAME=libnccl
ENV NV_LIBNCCL_PACKAGE_VERSION=${NV_LIBNCCL_PACKAGE_VERSION}
ENV NV_LIBNCCL_VERSION=${NV_LIBNCCL_VERSION}
ENV NCCL_VERSION=${NV_LIBNCCL_VERSION}
ENV NV_LIBNCCL_PACKAGE=${NV_LIBNCCL_PACKAGE_NAME}-${NV_LIBNCCL_PACKAGE_VERSION}+cuda12.9
ENV NV_CUDNN_VERSION=${NV_CUDNN_VERSION}
ENV NV_CUDNN_PACKAGE=libcudnn9-cuda-12-${NV_CUDNN_VERSION}

# -----------------------------------------------------------------------------
# NVIDIA Repository Setup
# -----------------------------------------------------------------------------
RUN --mount=type=cache,target=/var/cache/dnf,sharing=locked,id=odh-base-containers-cuda-dnf \
    dnf install -y --setopt=keepcache=1 --allowerasing curl ca-certificates

# Download and verify GPG key before creating repo
RUN NVIDIA_GPGKEY_SUM=d0664fbbdb8c32356d45de36c5984617217b2d0bef41b93ccecd326ba3b80c87 && \
    curl -fsSL "https://developer.download.nvidia.com/compute/cuda/repos/rhel9/${NVARCH}/D42D0685.pub" | sed '/^Version/d' > /etc/pki/rpm-gpg/RPM-GPG-KEY-NVIDIA && \
    echo "$NVIDIA_GPGKEY_SUM  /etc/pki/rpm-gpg/RPM-GPG-KEY-NVIDIA" | sha256sum -c --strict -

# Create repo config pointing to locally verified key
RUN { echo "[cuda-rhel9-${NVARCH}]"; \
      echo "name=cuda-rhel9-${NVARCH}"; \
      echo "baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel9/${NVARCH}"; \
      echo "enabled=1"; \
      echo "gpgcheck=1"; \
      echo "gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-NVIDIA"; \
    } > /etc/yum.repos.d/cuda.repo

# -----------------------------------------------------------------------------
# CUDA Base Packages
# -----------------------------------------------------------------------------
RUN --mount=type=cache,target=/var/cache/dnf,sharing=locked,id=odh-base-containers-cuda-dnf \
    dnf upgrade -y --setopt=keepcache=1 && \
    dnf install -y --setopt=keepcache=1 \
        cuda-cudart-12-9-${NV_CUDA_CUDART_VERSION} \
        cuda-compat-12-9

# Configure NVIDIA libraries path (nvidia-docker 1.0 compatibility)
RUN echo "/usr/local/nvidia/lib" >> /etc/ld.so.conf.d/nvidia.conf && \
    echo "/usr/local/nvidia/lib64" >> /etc/ld.so.conf.d/nvidia.conf

ENV PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:${PATH}
ENV LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64

# nvidia-container-runtime settings
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility,video

# Disable JIT caching for reproducibility (matches downstream)
# https://developer.nvidia.com/blog/cuda-pro-tip-understand-fat-binaries-jit-caching/
ENV CUDA_CACHE_DISABLE=1

# Load kernels on demand for faster startup
ENV CUDA_MODULE_LOADING=LAZY

# -----------------------------------------------------------------------------
# CUDA Runtime Libraries
# Source: https://gitlab.com/nvidia/container-images/cuda/-/blob/master/dist
# -----------------------------------------------------------------------------
RUN --mount=type=cache,target=/var/cache/dnf,sharing=locked,id=odh-base-containers-cuda-dnf \
    dnf install -y --setopt=keepcache=1 \
        cuda-libraries-12-9-${NV_CUDA_LIB_VERSION} \
        cuda-nvtx-12-9-${NV_NVTX_VERSION} \
        ${NV_LIBNPP_PACKAGE} \
        libcublas-12-9-${NV_LIBCUBLAS_VERSION} \
        ${NV_LIBNCCL_PACKAGE}

# -----------------------------------------------------------------------------
# Development Tools
# -----------------------------------------------------------------------------
RUN --mount=type=cache,target=/var/cache/dnf,sharing=locked,id=odh-base-containers-cuda-dnf \
    dnf install -y --setopt=keepcache=1 make findutils

# -----------------------------------------------------------------------------
# cuDNN
# Source: https://gitlab.com/nvidia/container-images/cuda/-/blob/master/dist
# -----------------------------------------------------------------------------
RUN --mount=type=cache,target=/var/cache/dnf,sharing=locked,id=odh-base-containers-cuda-dnf \
    dnf install -y --setopt=keepcache=1 ${NV_CUDNN_PACKAGE}

ENV XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda

# -----------------------------------------------------------------------------
# CUDA Toolkit
# -----------------------------------------------------------------------------
RUN --mount=type=cache,target=/var/cache/dnf,sharing=locked,id=odh-base-containers-cuda-dnf \
    dnf -y install --setopt=keepcache=1 cuda-toolkit-12-9

# -----------------------------------------------------------------------------
# Environment (consistent with Python base image)
# -----------------------------------------------------------------------------

# Python settings - prevent bytecode files and enable unbuffered output
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    UV_SYSTEM_PYTHON=1 \
    UV_NO_CACHE=1

# -----------------------------------------------------------------------------
# Package Index Configuration
# -----------------------------------------------------------------------------
# Configure pip and uv to use the specified package indexes.
# Users can simply run: pip install <pkg> or uv pip install <pkg>
# Override indexes at build time via --build-arg for downstream builds.

ARG PIP_INDEX_URL=https://pypi.org/simple
ARG PIP_EXTRA_INDEX_URL

# pip configuration (system-wide) - must be created before installing uv
RUN mkdir -p /etc/pip && \
    { echo "[global]"; \
      echo "index-url = ${PIP_INDEX_URL}"; \
      [ -n "${PIP_EXTRA_INDEX_URL}" ] && echo "extra-index-url = ${PIP_EXTRA_INDEX_URL}"; \
      true; \
    } > /etc/pip.conf

# Install uv - fast Python package installer
# https://github.com/astral-sh/uv
# Version pinned in requirements-build.txt
COPY --chmod=644 --chown=0:0 requirements-build.txt /tmp/requirements-build.txt
RUN python -m pip install --no-cache-dir -r /tmp/requirements-build.txt && \
    rm /tmp/requirements-build.txt

# uv configuration (system-wide)
RUN mkdir -p /etc/uv && \
    { echo "[pip]"; \
      echo "index-url = \"${PIP_INDEX_URL}\""; \
      [ -n "${PIP_EXTRA_INDEX_URL}" ] && echo "extra-index-url = [\"${PIP_EXTRA_INDEX_URL}\"]"; \
      true; \
    } > /etc/uv/uv.toml
ENV UV_CONFIG_FILE=/etc/uv/uv.toml

# Copy fix-permissions script from sclorg for OpenShift compatibility
# Source: https://github.com/sclorg/container-common-scripts
COPY --chmod=755 --chown=0:0 scripts/fix-permissions /usr/local/bin/fix-permissions

# -----------------------------------------------------------------------------
# Directory Setup (from notebooks + trustyai patterns)
# -----------------------------------------------------------------------------

# Create cache directories with proper permissions for OpenShift
# fix-permissions ensures group 0 can read/write, enabling arbitrary UID
RUN mkdir -p /opt/app-root/src/.cache && \
    chown -R 1001:0 /opt/app-root && \
    fix-permissions /opt/app-root -P

# -----------------------------------------------------------------------------
# Final Cleanup
# -----------------------------------------------------------------------------
RUN dnf clean all && \
    rm -rf /var/cache/dnf /var/log/dnf* /var/log/hawkey*

# -----------------------------------------------------------------------------
# Labels
# -----------------------------------------------------------------------------
ARG BUILD_DATE
ARG VCS_REF
ARG VERSION
ARG PYTHON_VERSION

LABEL name="odh-midstream-cuda-base" \
      version="${VERSION}" \
      summary="ODH CUDA Base Image" \
      description="CentOS Stream 9 with CUDA and Python for Open Data Hub workloads" \
      io.k8s.display-name="ODH CUDA Base" \
      io.k8s.description="CentOS Stream 9 with CUDA and Python for Open Data Hub workloads" \
      io.openshift.tags="cuda,python,ai,ml,gpu,c9s" \
      org.opencontainers.image.created="${BUILD_DATE}" \
      org.opencontainers.image.revision="${VCS_REF}" \
      org.opencontainers.image.version="${VERSION}" \
      org.opencontainers.image.title="ODH CUDA Base Image" \
      org.opencontainers.image.description="CentOS Stream 9 with CUDA and Python for Open Data Hub workloads" \
      org.opencontainers.image.source="https://github.com/opendatahub-io/base-containers" \
      org.opencontainers.image.vendor="Open Data Hub" \
      org.opencontainers.image.licenses="Apache-2.0" \
      com.nvidia.cuda.version="${CUDA_VERSION}" \
      com.nvidia.cudnn.version="${NV_CUDNN_VERSION}" \
      com.nvidia.nccl.version="${NCCL_VERSION}" \
      com.opendatahub.accelerator="cuda" \
      com.opendatahub.python="${PYTHON_VERSION}"

# -----------------------------------------------------------------------------
# User Configuration (from notebooks pattern)
# -----------------------------------------------------------------------------

# Switch to non-root user
# UID 1001 is standard for sclorg Python images and OpenShift SCC compatible
USER 1001

# Standard workdir for Python images
WORKDIR /opt/app-root/src

# -----------------------------------------------------------------------------
# Health Check
# -----------------------------------------------------------------------------
# Child images can add a HEALTHCHECK - example pattern:
# HEALTHCHECK --interval=30s --timeout=5s --retries=3 \
#     CMD python -c "import myapp; myapp.health_check()" || exit 1

# -----------------------------------------------------------------------------
# Extending This Image
# -----------------------------------------------------------------------------
#
# Example Dockerfile that extends this base:
#
#   FROM odh-midstream-cuda-base:12.9-py312
#
#   # Install dependencies (pip and uv are pre-configured with package indexes)
#   COPY requirements.txt .
#   RUN pip install -r requirements.txt
#   # Or with uv (faster):
#   # RUN uv pip install -r requirements.txt
#
#   # Copy application (preserve ownership for non-root user)
#   COPY --chown=1001:0 . .
#
#   # Run application
#   CMD ["python", "app.py"]
#
# -----------------------------------------------------------------------------
#
# Build for ODH (uses default package indexes):
#   podman build -t myapp:odh .
#
# Build for RHOAI (override to use internal mirror):
#   podman build -t myapp:rhoai \
#     --build-arg PIP_INDEX_URL=https://aipcc.internal/simple \
#     --build-arg PIP_EXTRA_INDEX_URL="" \
#     .
#
# -----------------------------------------------------------------------------
